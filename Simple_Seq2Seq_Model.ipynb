{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Sequence-to-Sequence Model - Step by Step\n",
    "\n",
    "This notebook provides a simplified, easy-to-understand implementation of a seq2seq model for translation.\n",
    "\n",
    "**What we'll build:** A German → English translator\n",
    "\n",
    "**Architecture:** Encoder-Decoder with RNN (GRU cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [Understanding Seq2Seq Models](#understanding)\n",
    "3. [Data Preparation](#data)\n",
    "4. [Building the Encoder](#encoder)\n",
    "5. [Building the Decoder](#decoder)\n",
    "6. [Complete Seq2Seq Model](#model)\n",
    "7. [Training](#training)\n",
    "8. [Translation & Evaluation](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Setup & Installation <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Understanding Seq2Seq Models <a id='understanding'></a>\n",
    "\n",
    "### What is a Sequence-to-Sequence Model?\n",
    "\n",
    "A seq2seq model transforms one sequence (input) into another sequence (output).\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input Sentence → [ENCODER] → Context Vector → [DECODER] → Output Sentence\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- Input: \"Hallo Welt\" (German)\n",
    "- Encoder: Reads and compresses the meaning\n",
    "- Context: Fixed-size representation of the input\n",
    "- Decoder: Generates \"Hello World\" (English)\n",
    "\n",
    "**Key Components:**\n",
    "1. **Encoder**: Reads input sequence and creates a context vector\n",
    "2. **Decoder**: Uses context to generate output sequence\n",
    "3. **Hidden State**: Carries information through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Data Preparation <a id='data'></a>\n",
    "\n",
    "We'll use the Multi30k dataset (German-English sentence pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers for German and English\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenizer functions\n",
    "def tokenize_de(text):\n",
    "    \"\"\"Convert German text to list of tokens\"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"Convert English text to list of tokens\"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Test tokenizers\n",
    "print(\"German:\", tokenize_de(\"Hallo Welt\"))\n",
    "print(\"English:\", tokenize_en(\"Hello World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Load the dataset\n",
    "print(\"Loading Multi30k dataset...\")\n",
    "train_data = list(Multi30k(split='train', language_pair=('de', 'en')))\n",
    "valid_data = list(Multi30k(split='valid', language_pair=('de', 'en')))\n",
    "test_data = list(Multi30k(split='test', language_pair=('de', 'en')))\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Valid size: {len(valid_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample pair:\")\n",
    "print(f\"German: {train_data[0][0]}\")\n",
    "print(f\"English: {train_data[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Build vocabularies\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        # Special tokens\n",
    "        self.itos = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "        self.stoi = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list, tokenizer):\n",
    "        \"\"\"Build vocabulary from list of sentences\"\"\"\n",
    "        frequencies = Counter()\n",
    "        idx = 4  # Start after special tokens\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for sentence in sentence_list:\n",
    "            for word in tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "        \n",
    "        # Add words that appear more than threshold\n",
    "        for word, count in frequencies.items():\n",
    "            if count >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "    \n",
    "    def numericalize(self, text, tokenizer):\n",
    "        \"\"\"Convert text to list of indices\"\"\"\n",
    "        tokenized = tokenizer(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokenized]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "# Create vocabularies\n",
    "print(\"Building vocabularies...\")\n",
    "german_vocab = Vocabulary(freq_threshold=2)\n",
    "english_vocab = Vocabulary(freq_threshold=2)\n",
    "\n",
    "# Build from training data\n",
    "german_sentences = [pair[0] for pair in train_data]\n",
    "english_sentences = [pair[1] for pair in train_data]\n",
    "\n",
    "german_vocab.build_vocabulary(german_sentences, tokenize_de)\n",
    "english_vocab.build_vocabulary(english_sentences, tokenize_en)\n",
    "\n",
    "print(f\"German vocabulary size: {len(german_vocab)}\")\n",
    "print(f\"English vocabulary size: {len(english_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.3: Create DataLoader\n",
    "class TranslationDataset:\n",
    "    def __init__(self, data, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_text, trg_text = self.data[idx]\n",
    "        \n",
    "        # Convert to indices\n",
    "        src_indices = [self.src_vocab.stoi[\"<sos>\"]] + \\\n",
    "                     self.src_vocab.numericalize(src_text, self.src_tokenizer) + \\\n",
    "                     [self.src_vocab.stoi[\"<eos>\"]]\n",
    "        \n",
    "        trg_indices = [self.trg_vocab.stoi[\"<sos>\"]] + \\\n",
    "                     self.trg_vocab.numericalize(trg_text, self.trg_tokenizer) + \\\n",
    "                     [self.trg_vocab.stoi[\"<eos>\"]]\n",
    "        \n",
    "        return torch.tensor(src_indices), torch.tensor(trg_indices)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad sequences in a batch to same length\"\"\"\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=0)  # pad_idx = 0\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=0)\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TranslationDataset(train_data, german_vocab, english_vocab, \n",
    "                                  tokenize_de, tokenize_en)\n",
    "valid_dataset = TranslationDataset(valid_data, german_vocab, english_vocab, \n",
    "                                  tokenize_de, tokenize_en)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                         shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, \n",
    "                         shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Building the Encoder <a id='encoder'></a>\n",
    "\n",
    "**What does the Encoder do?**\n",
    "- Takes input sentence (as word indices)\n",
    "- Converts words to embeddings\n",
    "- Processes them through RNN (GRU)\n",
    "- Outputs: final hidden state (context vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Size of source vocabulary\n",
    "            embedding_size: Dimension of word embeddings\n",
    "            hidden_size: Dimension of hidden state\n",
    "            num_layers: Number of RNN layers\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer: converts word indices to dense vectors\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        # GRU layer: processes sequence\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, \n",
    "                         dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (seq_len, batch_size)\n",
    "        \n",
    "        Returns:\n",
    "            hidden: Final hidden state (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # x shape: (seq_len, batch_size)\n",
    "        \n",
    "        # Step 1: Convert indices to embeddings\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_len, batch_size, embedding_size)\n",
    "        \n",
    "        # Step 2: Pass through RNN\n",
    "        outputs, hidden = self.rnn(embedding)\n",
    "        # outputs shape: (seq_len, batch_size, hidden_size)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "# Test the encoder\n",
    "encoder = Encoder(\n",
    "    input_size=len(german_vocab),\n",
    "    embedding_size=256,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(\"Encoder created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in encoder.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Building the Decoder <a id='decoder'></a>\n",
    "\n",
    "**What does the Decoder do?**\n",
    "- Takes: previous word + hidden state from encoder\n",
    "- Predicts: next word in output sequence\n",
    "- Repeats until `<eos>` token is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output_size: Size of target vocabulary\n",
    "            embedding_size: Dimension of word embeddings\n",
    "            hidden_size: Dimension of hidden state (must match encoder)\n",
    "            num_layers: Number of RNN layers (must match encoder)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Embedding layer for target language\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        \n",
    "        # GRU layer\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers,\n",
    "                         dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Output layer: projects hidden state to vocabulary size\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (1, batch_size) - one word at a time\n",
    "            hidden: Hidden state from encoder or previous step\n",
    "        \n",
    "        Returns:\n",
    "            prediction: Output logits (batch_size, output_size)\n",
    "            hidden: Updated hidden state\n",
    "        \"\"\"\n",
    "        # x shape: (1, batch_size)\n",
    "        \n",
    "        # Step 1: Get embedding\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, batch_size, embedding_size)\n",
    "        \n",
    "        # Step 2: Pass through RNN\n",
    "        output, hidden = self.rnn(embedding, hidden)\n",
    "        # output shape: (1, batch_size, hidden_size)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
    "        \n",
    "        # Step 3: Project to vocabulary size\n",
    "        prediction = self.fc(output.squeeze(0))\n",
    "        # prediction shape: (batch_size, output_size)\n",
    "        \n",
    "        return prediction, hidden\n",
    "\n",
    "# Test the decoder\n",
    "decoder = Decoder(\n",
    "    output_size=len(english_vocab),\n",
    "    embedding_size=256,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(\"Decoder created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in decoder.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Complete Seq2Seq Model <a id='model'></a>\n",
    "\n",
    "**How it works:**\n",
    "1. Encoder processes source sentence\n",
    "2. Decoder generates target sentence word-by-word\n",
    "3. Teacher forcing: sometimes use true target word during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Source sequence (src_len, batch_size)\n",
    "            trg: Target sequence (trg_len, batch_size)\n",
    "            teacher_forcing_ratio: Probability of using true target word\n",
    "        \n",
    "        Returns:\n",
    "            outputs: Predictions (trg_len, batch_size, output_size)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # Store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Step 1: Encode source sentence\n",
    "        hidden = self.encoder(src)\n",
    "        \n",
    "        # Step 2: First input to decoder is <sos> token\n",
    "        input = trg[0, :].unsqueeze(0)  # Shape: (1, batch_size)\n",
    "        \n",
    "        # Step 3: Generate output sequence word by word\n",
    "        for t in range(1, trg_len):\n",
    "            # Get prediction from decoder\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            \n",
    "            # Store prediction\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # Decide next input: true word or predicted word?\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            # Next input is either true word or predicted word\n",
    "            input = trg[t].unsqueeze(0) if teacher_force else top1.unsqueeze(0)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Create the complete model\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "print(\"\\nSeq2Seq Model created!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Training <a id='training'></a>\n",
    "\n",
    "**Training Process:**\n",
    "1. Forward pass: Get predictions\n",
    "2. Calculate loss: Compare predictions to true targets\n",
    "3. Backward pass: Compute gradients\n",
    "4. Update weights: Use optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "\n",
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # Calculate loss\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)  # Ignore <sos> token\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            # No teacher forcing during evaluation\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)\n",
    "            \n",
    "            # Calculate loss\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    # Save best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    train_ppl = math.exp(train_loss)\n",
    "    valid_ppl = math.exp(valid_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:7.3f}')\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Translation & Evaluation <a id='evaluation'></a>\n",
    "\n",
    "Now let's use our trained model to translate sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, src_vocab, trg_vocab, src_tokenizer, max_len=50):\n",
    "    \"\"\"\n",
    "    Translate a German sentence to English\n",
    "    \n",
    "    Args:\n",
    "        model: Trained seq2seq model\n",
    "        sentence: German sentence (string)\n",
    "        src_vocab: German vocabulary\n",
    "        trg_vocab: English vocabulary\n",
    "        src_tokenizer: German tokenizer\n",
    "        max_len: Maximum length of translation\n",
    "    \n",
    "    Returns:\n",
    "        translation: English sentence (string)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and convert to indices\n",
    "    tokens = [src_vocab.stoi[\"<sos>\"]] + \\\n",
    "             src_vocab.numericalize(sentence, src_tokenizer) + \\\n",
    "             [src_vocab.stoi[\"<eos>\"]]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode source\n",
    "        hidden = model.encoder(src_tensor)\n",
    "        \n",
    "        # Start with <sos> token\n",
    "        trg_indices = [trg_vocab.stoi[\"<sos>\"]]\n",
    "        \n",
    "        # Generate translation word by word\n",
    "        for _ in range(max_len):\n",
    "            trg_tensor = torch.LongTensor([trg_indices[-1]]).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            output, hidden = model.decoder(trg_tensor, hidden)\n",
    "            \n",
    "            # Get most likely next word\n",
    "            pred_token = output.argmax(1).item()\n",
    "            trg_indices.append(pred_token)\n",
    "            \n",
    "            # Stop if <eos> token is generated\n",
    "            if pred_token == trg_vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "    \n",
    "    # Convert indices to words\n",
    "    trg_tokens = [trg_vocab.itos[i] for i in trg_indices]\n",
    "    \n",
    "    # Remove special tokens and join\n",
    "    translation = ' '.join(trg_tokens[1:-1])  # Remove <sos> and <eos>\n",
    "    \n",
    "    return translation\n",
    "\n",
    "print(\"Translation function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Test translations\n",
    "test_sentences = [\n",
    "    \"Ein Mann geht die Straße entlang.\",\n",
    "    \"Eine Frau spielt mit einem Kind.\",\n",
    "    \"Der Hund läuft im Park.\",\n",
    "    \"Zwei Männer spielen Fußball.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRANSLATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translation = translate_sentence(\n",
    "        model, sentence, german_vocab, english_vocab, tokenize_de\n",
    "    )\n",
    "    print(f\"\\nGerman:  {sentence}\")\n",
    "    print(f\"English: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "def calculate_bleu(model, data, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer, max_samples=100):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score on dataset\n",
    "    \"\"\"\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    for i, (src, trg) in enumerate(data[:max_samples]):\n",
    "        # Generate translation\n",
    "        translation = translate_sentence(\n",
    "            model, src, src_vocab, trg_vocab, src_tokenizer\n",
    "        )\n",
    "        \n",
    "        # Tokenize reference and hypothesis\n",
    "        ref = trg_tokenizer(trg)\n",
    "        hyp = translation.split()\n",
    "        \n",
    "        references.append([ref])\n",
    "        hypotheses.append(hyp)\n",
    "    \n",
    "    # Calculate corpus BLEU score\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    return bleu_score\n",
    "\n",
    "# Calculate BLEU score on test set\n",
    "bleu = calculate_bleu(model, test_data, german_vocab, english_vocab, \n",
    "                     tokenize_de, tokenize_en, max_samples=100)\n",
    "print(f\"\\nBLEU Score: {bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**What we built:**\n",
    "1. ✅ Encoder: Processes source sentence\n",
    "2. ✅ Decoder: Generates target sentence\n",
    "3. ✅ Seq2Seq Model: Combines both\n",
    "4. ✅ Training Loop: Learns from data\n",
    "5. ✅ Translation Function: Generates translations\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Embedding**: Converts words to vectors\n",
    "- **RNN/GRU**: Processes sequences\n",
    "- **Hidden State**: Carries information\n",
    "- **Teacher Forcing**: Training technique\n",
    "- **Perplexity**: Measures model uncertainty\n",
    "- **BLEU Score**: Measures translation quality\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different hyperparameters\n",
    "- Add attention mechanism\n",
    "- Use LSTM instead of GRU\n",
    "- Train on larger dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise: Translate Your Own Sentences!\n",
    "\n",
    "Try translating your own German sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Try translating these:\n",
    "my_sentences = [\n",
    "    \"Guten Morgen!\",\n",
    "    \"Wie geht es dir?\",\n",
    "    \"Ich liebe Programmieren.\",\n",
    "    # Add your own sentences here!\n",
    "]\n",
    "\n",
    "for sentence in my_sentences:\n",
    "    translation = translate_sentence(\n",
    "        model, sentence, german_vocab, english_vocab, tokenize_de\n",
    "    )\n",
    "    print(f\"German:  {sentence}\")\n",
    "    print(f\"English: {translation}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
